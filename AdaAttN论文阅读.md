# AdaAttN论文阅读

## 一、标题

​	`AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer  ` 

​	即：$$AdaAttN$$：重新审视任意神经风格迁移中的注意力机制。从标题中可以知道，本论文的核心便是风格迁移任务中的注意力机制。

## 二、概要（Abstract）

### 2.1 内容

​	先介绍了当前风格迁移任务的方法：要么专注地将深度风格特征融合到深度内容特征中，而不考虑特征分布，要么根据风格自适应地规范化深度内容特征，以使它们的全局统计数据匹配（`Existing solutions either attentively fuse deep style feature into deep content feature without considering feature distributions, or adaptively normalize deep content feature according to the style such that their global statistics are matched.`）。

​	再指出存在的问题：尽管这些方法有效，但对浅层特征未加探索，并且没有局部考虑特征统计，容易导致带有局部失真的不自然输出（`Although effective, leaving shallow feature unexplored and without locally considering feature statistics, they are prone to unnatural output with unpleasing local distortions. `）。

​	论文提出的解决方案：提出了一个新颖的注意力和规范化模块，名为自适应注意力规范化（`AdaAttN`），以逐点自适应地执行注意力规范化（`To alleviate this problem, in this paper, we propose a novel attention and normalization module, named Adaptive Attention Normalization (AdaAttN), to adaptively perform attentive normalization on per-point basis.`）。

方案的具体内容：

- 空间注意力分数是从内容和风格图像的浅层和深层特征中学习得来的（`Specifically, spatial attention score is learnt from both shallow and deep features of content and style images.  `）。
- 然后，通过将风格特征点视为所有风格特征点的注意力加权输出的分布，计算每个点的加权统计数据（`Then perpoint weighted statistics are calculated by regarding a style feature point as a distribution of attention-weighted output of all style feature points.`）。
- 最后，对内容特征进行规范化，以使其展现出与计算得到的每个点加权风格特征统计数据相同的局部特征统计（`Finally, the content feature is normalized so that they demonstrate the same local feature statistics as the calculated per-point weighted style feature statistics.  `）。
- 除此之外，基于`AdaAttN`提出了一种新颖的局部特征损失，用以增强局部视觉质量（`Besides, a novel local feature loss is derived based on AdaAttN to enhance local visual quality.  `）。

  论文还对`AdaAttN`进行了轻微修改，以便应用于视频风格转移。

### 2.2 相关概念理解

#### 2.2.1 浅层特征与深层特征（`shallow feature, deep feature`）







#### 2.2.2 统计（`statistics`）







#### 2.2.3 注意力机制（`Attention Mechanism`）









#### 2.2.4 规范化（`normalize`）











## 三、介绍（`Introduce`）

### 3.1 内容

​	首段介绍了风格迁移的大致发展过程。风格迁移任务的目标是将风格图像的模式应用到内容图像中，并同时保留内容图像的结构。最为影响深远的工作由`Gatys  `等人完成，他们提出了一种图像优化方法，该方法在预训练深度神经网络的特征空间中迭代地最小化内容损失和风格损失。然而，这种耗时的优化过程促使研究人员探索更有效的方法。`Johnson  `等人提出使用前向网络直接生成渲染图像，实现了实时风格转移。因为学习好的模型只能对一种特定风格生效，这个方法与其相关的工作被分类为`Per-Style-Per-Model   `方法。除此之外，还有`Multiple-Style-Per-Model   `方法和`Arbitrary-Style-Per-Model   `方法。在后一种情况下，一旦模型训练完成，该模型可以接受任何风格图像作为输入，并在单次前向传递中生成风格化结果。

​	第二段介绍风格迁移中存在的问题，即：增加灵活性牺牲了对于任意风格转移网络的局部风格模式建模能力。比如$$AdaIN$$方法，它在特征空间将风格图像的全局均值和方差转移给内容图像，以支持任意输入的风格图像。由于特征的均值和方差是全局计算的，局部细节和逐点模式很大程度上被忽略，因此局部样式化性能大幅下降。在`[5, 22, 15, 24, 10]`中也存在类似的灵活性和能力之间的权衡，其中内容图像的所有局部特征点都通过相同的基于风格图像的变换函数进行处理。为了增强任意风格转移模型对局部特征的认知，最近多项工作（`[28, 6, 43]`）在这一任务中采用了注意力机制。它们的共同直觉是，模型在将内容图像区域进行风格化时应更多关注风格图像中特征相似的区域。这种注意力机制已被证明对于在任意风格转移中生成更多的局部风格细节是有效的。不幸的是，虽然提升了性能，但并未完全解决这个问题，局部失真仍然会发生。

​	第三段解释为什么会有这种困境。论文认为揭示上述给注意力机制带来困境的原因并不是一件很困难的事情。深入研究当前基于注意力的任意风格转移解决方案的细节，很容易发现：                                                                                                                                	              	1）设计的注意力机制通常基于深度卷积神经网络在较高抽象级别上的特征，忽略了低层次的细节；                                                     	2）注意力分数通常用于重新加权风格图像的特征图，然后将重新加权的风格特征简单地融合到内容特征中进行解码。                                       基于深度卷积神经网络特征的注意力策略忽略了浅层网络中图像的低层模式。因此，注意力分数可能很少关注低级纹理，并且受到高级语义的主导。与此同时，就像`SANet`中做的一样，对风格特征进行空间重新加权，然后将重新加权的风格特征与内容特征融合，而不考虑特征分布。

​	第四段提出了论文的解决方案。论文尝试解决这些问题，并在风格模式转移和内容结构保留之间取得更好的平衡。受到以上分析所得的经验教训的启发，论文提出了一个新颖的注意力和规范化模块，名为自适应注意力规范化（$$AdaAttN$$），用于任意风格转移。它可以根据特征分布进行逐点基础的自适应注意力规范化，更详细地说，它从内容和风格图像的浅层和深层特征中学习空间注意力分数。然后，通过将风格特征点视为所有空间特征点的注意力加权输出的分布，计算每个点的加权统计数据。最后，对内容特征进行规范化，使其局部特征统计与每个点加权风格特征的统计数据相同。这种方式下，注意力模块考虑了风格图像和内容图像的浅层和深层卷积神经网络特征。同时，实现了从内容特征到调制风格特征的逐点特征统计对齐。基于$$AdaAttN$$模块，提出了一种新的优化目标，名为局部特征损失，并推导出了一个新的任意图像风格转移流程。论文的贡献主要如下：

- 我们引入了一种新颖的$$AdaAttN$$模块，用于任意风格转移。它考虑了浅层和深层特征用于注意力分数的计算，并适当地对内容特征进行规范化，使特征统计与基于注意力加权的风格特征的均值和方差图在每个点上都能很好地对齐。
- 提出了一种新的优化目标，称为局部特征损失。它有助于模型训练，并通过规范生成图像的局部特征来提高任意风格转移的质量。
- 进行了大量实验并与其他最先进的方法进行了比较，以展示提出的方法的有效性。
- 通过引入基于余弦距离的注意力和基于图像相似度的损失，进一步扩展我们的模型用于视频风格转移可以产生稳定且吸引人的结果。

### 3.2 相关概念理解



















## 四、相关工作（`Arbitrary Style Transfer  `）

### 4.1 任意风格迁移（`Arbitrary Style Transfer  `）

​	最近的任意风格转移方法可以分为两类：基于全局转换和基于局部转换的方法。前者的共同思想是在全局范围应用特征修改。`WCT `通过包括白化和着色在内的两个转换步骤实现了这一点。`Huang`等人提出了`AdaIN`，它自适应地应用每个风格特征的均值和标准差来移动和重新缩放相应的归一化内容特征，以使内容特征和风格特征共享相同的分布。`Jing`等人通过动态实例归一化扩展了这种方法，其中中间卷积块的权重由另一个以风格图像为输入的网络生成。`Li`等人提出根据内容和风格特征生成线性转换。此外，`Deng`等人使用多通道相关性得到了转换函数。尽管这些方法完成了整体的任意风格转移任务，并在这一领域取得了很大进展，但由于它们所利用的全局转换难以处理详细的局部信息，因此局部风格转移性能通常不尽人意。

​	对于后一类方法，`Chen`等人提出了一种风格交换方法，这是一种基于补丁的风格转移方法，依赖于内容和风格补丁之间的相似性。`[11]`是另一种基于补丁的方法，考虑了全局统计和局部补丁的匹配。`Avatar-Net `进一步提出了一个多尺度框架，结合了风格交换和`AdaIN`功能的思想。近年来，注意力机制广泛应用于任意风格转移，因为它在对风格和内容图像的局部特征进行精细对应建模方面具有出色的能力。在这一流程中，`Park`等人提出了风格注意力网络（`SANet`），用于匹配内容和风格特征。`Yao`等人在这种注意力框架下考虑了不同类型的笔触。`Deng`等人提出了一个多适应模块，对内容特征应用逐点注意力，对风格特征应用通道级别的注意力。这些方法通常采用的常见做法是仅基于深度卷积神经网络特征构建注意力机制，而不考虑浅层特征，并简单地混合内容特征和重新加权的风格特征。因此，这往往会在很大程度上扭曲原始的内容结构，并导致人眼不愉快的效果。在本文中，我们的目标是探索在风格模式转移和内容结构保留之间寻求更好的平衡。

### 4.2 视频风格迁移（`Video Style Transfer  `）

​	直接将图像风格转移技术应用于视频帧序列通常会导致由时间不一致性引起的闪烁效果。因此，许多工作在原始图像风格转移方案中添加了光流一致性约束，例如，用于基于优化的视频风格转移的`[30]`，用于每个风格每个模型方法的`[31, 1, 12, 13, 8]`，用于任意风格每个模型方法的`[37, 38]`，以及用于图像到图像翻译框架的`[36, 4, 26]`。光流约束提高了视频风格转移的稳定性。然而，它严重依赖于高精度的预提取光流场，以进行基于流的扭曲。一些工作针对稳定性问题采用了除光流扭曲之外的其他方法来解决。`[22, 5]`利用转换模型的线性性质来保证特征空间上的帧间一致性。吴等人`[41]`提出了一种基于`SANet`的方法，借助`SSIM`一致性约束使当前帧关注于与前一帧相似的区域。与这些方法不同，在这项工作中，我们利用注意力机制增加了一种新颖的图像级相似性损失，以克服闪烁现象，并且在没有先验光流的情况下实现了可比较甚至更好的稳定性。

## 五、方法

### 5.1 总体框架（Overall Framework）

​	所提出的网络接收风格图像 $$I_s$$ 和内容图像 $$I_c$$ 来合成风格化图像 $$I_{cs}$$。在论文提出的模型中，使用预训练的 $$VGG-19$$ 网络作为编码器，提取多尺度特征图。解码器遵循的设置，采用了 $$VGG-19$$ 的对称结构。为了充分利用浅层和深层特征，论文采用了多层策略，分别在 $$VGG$$ 的 `ReLU-3_1`、`ReLU-4_1`和`ReLU-5_1`层上整合了三个 $$AdaAttN$$ 模块，如下图所示。论文将 $$VGG$$ 中的层`ReLU-x_1`提取的特征表示为$$F_*^x \in R^{H_* W_*} $$，其中当它以图像 $$I_*$$ 作为输入时，∗ 可以是 c 或 s，分别表示内容和风格特征。为了充分利用低层模式，论文进一步将当前层的特征与其前几层的下采样特征连接起来，形成：
$$
F_*^{1:x}=D_x(F_*^1)\oplus D_x(F_*^2)\oplus \cdots \oplus F^x_*
$$
其中，$$D_x$$ 代表双线性插值层，将输入特征下采样到与 $$F_*^x$$ 相同的形状，⊕ 表示沿着通道维度的连接操作。然后，可以将层 $$l$$ 的 $$AdaAttN$$ 模块的嵌入特征表示为：

$$
F_{cs}^x=AdaAttN(F^x_c,F^x_s,F^{1:x}_c,F_x^{1:x})
$$
其中，$$F_c$$、$$F_s$$和 $$F_{cs}$$ 分别是内容、风格和嵌入特征。利用多级嵌入特征，可以使用解码器合成风格化图像 $$I_{cs}$$，表达式如下：
$$
I_{cs}=Dec(F_{cs}^3,F_{cs}^4,F_{cs}^5)
$$
![image-20231204220456764](C:\Users\kwz\AppData\Roaming\Typora\typora-user-images\image-20231204220456764.png)

### 5.2 自适应注意力规范化（Adaptive Attention Normalization）

​	特征转换模块是任意风格转移模型中的关键组件。论文的模块与其他框架的比较如下图所示。

![image-20231205103012370](C:\Users\kwz\AppData\Roaming\Typora\typora-user-images\image-20231205103012370.png)

开创性的 $$AdaIN$$ 只考虑了整体的风格分布，并且调整内容特征，使其特征分布与风格特征的全局特征对齐。考虑到局部风格模式，$$SANet$$ 从风格和内容特征图计算注意力图，然后用注意力图调制风格特征，将注意力输出融合到内容特征中。$$SANet$$ 在局部风格化方面表现出色。然而，它缺乏低级匹配和局部特征分布对齐。受到从 $$AdaIN$$ 和 $$SANet$$ 中学到的经验启发，论文提出了自适应注意力规范化（$$AdaAttN$$）模块，通过考虑低级和高级特征，并结合注意力机制，能够自适应地在每个点上转移特征分布。正如上图所示，$$AdaAttN$$ 分为三个步骤：                                                                                                                                                                                   （1）利用浅层到深层的内容和风格特征计算注意力图；                                                                                                                                                                  （2）计算风格特征的加权平均值和标准方差图；                                                                                                                                                      （3）自适应地对内容特征进行归一化，以实现每个点的特征分布对齐。

​	**注意力图生成**。在任意风格转移方法中，注意力机制被用来衡量内容和风格特征之间的相似性。与之前仅使用相对深层特征的方法不同，我们同时使用内容和风格特征的低级和高级层。要计算层 $$x$$ 的注意力图 $$A$$，我们将 $$Q$$(query)，$$K$$(key) 和 $$V$$(value) 表示为：

$$
Q=f(Norm(F_c^{1:x})),\\
K=g(Norm(F_s^{1:x})),\\
V=h(F_s^x)
$$
这里的 $$f,g,$$ 和 $$h$$ 是 $$1 × 1$$ 可学习的卷积层，$$Norm$$表示通道方向上的均值-方差归一化，类似于实例归一化。注意力图 $$A$$ 可以计算如下：
$$
A=Softmax(Q^T \otimes K),
$$
这里 $$\otimes$$ 表示矩阵乘法。

​	**加权均值和标准方差图**。将注意力得分矩阵 $$A$$ 应用于风格特征 $$F_s^x$$，可以类似于 $$SANet$$ 的做法，被视为通过对所有风格特征点进行加权求和来计算每个目标风格特征点。论文解释这个过程为将通过注意力输出的目标风格特征点视为所有加权风格特征点的分布。然后，从这个角度来看，我们可以计算每个分布的统计量。我们将这些统计量分别称为注意力加权均值和注意力加权标准方差。因此，注意力加权均值 $$M \in R^{C \times H_c W_c}$$  可表示为：
$$
M=V \otimes A^T
$$
在此处 $$A \in R^{H_c W_c \times H_s W_s}$$，$$V \in R^{C \times H_c W_c}$$。因为变量的方差等于其平方的期望减去期望的平方，我们可以得到注意力加权标准差 $$S \in R^{C \times H_c W_c}$$ 如下：
$$
S=\sqrt{(V \cdot V)\otimes A^T - M\cdot M},
$$
这里的 · 表示逐元素相乘。

​	**自适应归一化**。最后，对于归一化的内容特征图的每个位置和每个通道，使用 $$S$$ 中的相应比例和$$M$$ 中的偏移量生成变换后的特征图：
$$
F_{cs}^x=S \cdot Norm(F_c^x)+M.
$$
简而言之，$$AdaAttN$$ 通过生成注意力加权均值和方差图执行特征统计量的转移。如上图所示，与$$AdaIN$$ 相比，$$AdaAttN$$ 考虑的是每个点的统计量，而不是全局统计量。$$AdaAttN$$ 比 $$AdaIN$$ 更通用。对于每个 $$i,j$$，如果设置 $$Ai,j = 1/(H_s W_s)$$，$$AdaAttN$$可以特化为$$AdaIN$$。与 $$SANet$$ 相比，注意力机制用于计算目标特征分布，而不是直接生成转移特征以进行进一步融合。

### 5.3 损失函数（Loss Function）

​	我们的整体损失函数是全局风格损失（$$\mathcal L_{gs}$$）和局部特征损失（$$\mathcal {L_{lf}}$$）的加权总和：
$$
\mathcal L = \lambda_g \mathcal L_{gs} + \lambda_l \mathcal L_{lf}
$$
其中 $$\mathcal \lambda_g$$ 和 $$\lambda_l$$ 是控制对应损失项权重的超参数。

​	首先，遵循[14]和许多其他工作，论文通过缩小在 $$VGG$$ 特征空间中生成图像和风格图像之间均值 µ 和标准差 σ 的距离，以确保全局的风格化效果（$$\mathcal L_{gs}$$）：
$$
\mathcal {L_{gs}} = \sum_{x=2}^5(||\mu(E^x(I_{cs}))-\mu(F_s^x)||_2+||\sigma(E^x(I_{cs}))-\sigma(F_s^x)||_2)
$$
这里的 $$E()$$ 表示 $$VGG$$ 编码器的特征，上标 $$^x$$ 表示层索引。

​	所提出的新型局部特征损失 $$\mathcal L_{lf}$$ 约束了风格化图像的特征图与 $$AdaAttN$$ 函数的转换结果保持一致：
$$
\mathcal L_{lf}=\sum_{x=3}^5||E_x(I_{cs})-AdaAttN^*(F^x_c,F^x_s,F^{1:x}_c,F_x^{1:x})||_2
$$
这里的 $$AdaAttN^*$$ 作为应该是确定性的监督信号。因此，我们考虑不带有三个可学习的 $$1 × 1$$ 卷积核（$$f$$、$$g$$ 和 $$h$$）的无参数版本的 $$AdaAttN$$。局部特征损失使得模型针对局部区域生成的风格化输出要比[14, 28]中使用的传统内容损失更好。

### 5.4 视频风格转移的扩展（Extension for Video Style Transfer）

​	与其他基于注意力的方法相比，论文的方法能够生成更自然的风格化结果，减少了局部失真，因此在视频风格转移方面具有巨大潜力。通过两个轻微的修改，论文的模型可以扩展到视频风格转移。

​	首先，$$A=Softmax(Q^T \otimes K)$$中的 $$Softmax$$ 函数由于指数计算具有强烈的排他性，它可以主要关注局部模式，并对稳定性产生负面影响。针对视频风格转移，论文考虑使用余弦相似度来计算注意力图：
$$
A_{i,j}=\frac{S_{i,j}}{\sum_jS_{i,j}},S_{i,j}=\frac{Q_i \cdot K_j}{||Q_i|| \times||K_j||}+1,
$$
余弦相似度可以获得比 $$Softmax$$ 更平坦的注意力分数分布。因此，局部特征统计将更稳定，局部风格模式不会被过分强调，从而更好地保证一致性。

​	其次，基于注意力机制，论文设计了一个新的跨图像相似性损失项 $$\mathcal L_{is}$$，以规范化两个内容图像 $$c_1,c_2$$ 之间的相关内容：
$$
\mathcal{L_{is}}=\sum^4_{x=2}\frac{1}{N^x_{c_1}N^x_{c_2}}\sum |\frac{D^{i,j,x}_{c_1,c_2}}{\sum_i D^{i,j,x}_{c_1,c_2}}-\frac{D_{cs_1,cs_2}^{i,j,x}}{\sum_i D^{i,j,k}_{cs_1,cs_2}}|,\\
D^{i,j,x}_{u,v}=1-\frac{F_u^{x,i}\cdot F_{v}^{x,j}}{||F_u^{x,i}||\times||F_v^{x,j}||}
$$
这里，$$N_c^x$$是 $$ReLU\frac{}{}x\_1$$ 层中内容特征图 $$F_c^x$$ 的空间维度大小，$$F_*^{x,i}$$表示 $$F_*^{x}$$ 的第 $$i$$ 个位置的特征向量，而 $$D^{i,j,x}_{u,v}$$ 衡量了 $$F_u^{x,i}$$ 和 $$F_v^{x,j}$$ 之间的余弦距离。在每个训练迭代中，会抽样两个输入视频帧以启用此损失。直观地说，这种跨图像相似性损失要求两个内容图像的风格化结果与两个原始图像共享类似的局部相似性模式。因此，它确保在视频风格转移中具有帧间关系意识，并有助于稳定结果。

## 六、实验（Experiments）

​	此部分不是我们所要重点关注的，所以不做仔细地研究，而只略读。
